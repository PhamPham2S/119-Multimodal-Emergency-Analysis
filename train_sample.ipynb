{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PhamPham2S/NewJeans-5/blob/main/train_sample.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Load Data, Library"
      ],
      "metadata": {
        "id": "KqX6uu5Y0L2Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUO9JNc00ACz",
        "outputId": "dc9ea783-56aa-4007-ab15-cee5fa1ea102"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchaudio transformers datasets"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ywq1aOgv0Vu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/PhamPham2S/NewJeans-5.git"
      ],
      "metadata": {
        "id": "yBlzmH7PNPi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "PROJECT_ROOT = Path(\"/content/drive/MyDrive/LikeLion/실전 프로젝트 1/Project\")\n",
        "SRC_ROOT = PROJECT_ROOT / \"src\"\n",
        "if str(SRC_ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(SRC_ROOT))\n"
      ],
      "metadata": {
        "id": "DcG5Qitj0LZv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"/content/drive/MyDrive/LikeLion/실전 프로젝트 1/Project\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gu7jvAXy0oSq",
        "outputId": "ae385c9d-5c77-4424-80a6-9467705d6043"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/13Uz1efdlntYkSUH521hpq0OZaC1fABeW/실전 프로젝트 1/Project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reimport\n",
        "import importlib\n",
        "importlib.reload(sys.modules['src.core.modeling'])\n",
        "\n",
        "# src/core 모듈 import\n",
        "from src.core.data_pipeline import build_embedding_dataloader\n",
        "from src.core.modeling import FusionModel_train as FusionModel\n",
        "from src.core.multitask import MultiTaskLoss, MultiTaskLossController\n",
        "from src.core.grad_monitor import GradMonitor\n",
        "from src.core.losses import ordinal_loss\n",
        "\n",
        "# 기타 PyTorch\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "dU7szH3Z96Nf"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Hyperparameter setting"
      ],
      "metadata": {
        "id": "NVX1A2l9-DuL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 2\n",
        "LEARNING_RATE = 1e-5\n",
        "\n",
        "# Sample 데이터 경로\n",
        "data_dir = Path(\"data/Sample\")"
      ],
      "metadata": {
        "id": "McAU1S2O-IGA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Data Loader"
      ],
      "metadata": {
        "id": "HXRJFf1F-naM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Audio Embedding load\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "\n",
        "# audio_root = Path(\"/content/drive/MyDrive/LikeLion/실전 프로젝트 1/Project/data/emb/aud_emb/embed-audio-hybrid\")\n",
        "audio_root = Path(\"/content/NewJeans-5/data/emb/10000_aud_emb\")\n",
        "\n",
        "audio_embeddings = []\n",
        "\n",
        "for npy_file in tqdm(audio_root.glob(\"*.npy\")):\n",
        "    emb = np.load(npy_file)\n",
        "    audio_embeddings.append(\n",
        "        {\n",
        "            \"emb\":emb,\n",
        "            \"id\": npy_file.stem\n",
        "        })\n",
        "\n",
        "print(f\"로드된 audio embedding 개수: {len(audio_embeddings)}\")\n",
        "# print(f\"예시 shape: {audio_embeddings[0].shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbE36U3qWXcx",
        "outputId": "36a97dcd-1fc9-4c35-8e3b-bee5e85c07fd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "10000it [00:00, 10457.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "로드된 audio embedding 개수: 10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Text Embedding load\n",
        "# txt_root = Path(\"/content/drive/MyDrive/LikeLion/실전 프로젝트 1/Project/data/emb/txt_emb\")\n",
        "txt_root = Path(\"/content/NewJeans-5/data/emb/txt_emd_data\")\n",
        "txt_embeddings = []\n",
        "\n",
        "for folder in txt_root.iterdir():\n",
        "    if folder.is_dir():\n",
        "        for npy_file in folder.iterdir():\n",
        "            if npy_file.is_file() and npy_file.suffix == \".npy\":\n",
        "                # print(npy_file)\n",
        "                # print(npy_file.stem)\n",
        "                emb = np.load(npy_file)   # 실제 npy 내부 데이터 로드\n",
        "                txt_embeddings.append(\n",
        "                    {\n",
        "                        \"emb\":emb,\n",
        "                        \"id\": npy_file.stem\n",
        "                    })\n",
        "\n",
        "print(f\"로드된 txt embedding 개수: {len(txt_embeddings)}\")\n",
        "# print(f\"예시 shape: {txt_embeddings[0].shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgQ94pisWbWu",
        "outputId": "734ea026-ce32-4f1e-969c-12cfdb33f280"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "로드된 txt embedding 개수: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Concat\n",
        "text_index = {\n",
        "    item[\"id\"]: item[\"emb\"]\n",
        "    for item in txt_embeddings\n",
        "}\n",
        "\n",
        "embeddings = {}\n",
        "\n",
        "for item in audio_embeddings:\n",
        "    id = item[\"id\"]\n",
        "\n",
        "    if id not in text_index:\n",
        "        continue  # or log\n",
        "\n",
        "    embeddings[id] = {\n",
        "        \"audio\": item[\"emb\"],\n",
        "        \"text\": text_index[id],\n",
        "    }\n",
        "\n",
        "print(f\"합쳐진 embedding 총 개수: {len(embeddings)}\")\n",
        "\n",
        "missing_text = [\n",
        "    item[\"id\"] for item in audio_embeddings\n",
        "    if item[\"id\"] not in text_index\n",
        "]\n",
        "\n",
        "print(\"text missing:\", len(missing_text))\n",
        "print(missing_text[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Lar-qJtckXZ",
        "outputId": "da78daf9-9033-4c4a-f942-037b5b5d62d8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "합쳐진 embedding 총 개수: 10000\n",
            "text missing: 0\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm"
      ],
      "metadata": {
        "id": "zPeB6UIsiDIu"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Labeling\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "json_root = Path(\"/content/drive/MyDrive/LikeLion/실전 프로젝트 1/Project/data/emb/json_emb\")\n",
        "\n",
        "def find_json_by_id(json_root: Path, id: str) -> Path | None:\n",
        "    \"\"\"\n",
        "    여러 폴더 중에서 {id}.json 파일 경로 반환\n",
        "    \"\"\"\n",
        "    for folder in json_root.iterdir():\n",
        "        # print(folder)\n",
        "        if not folder.is_dir():\n",
        "            continue\n",
        "\n",
        "        json_path = folder / f\"{id}.json\"\n",
        "        if json_path.exists():\n",
        "            # print(f\"JSON 파일 존재 : {json_path}\")\n",
        "            return json_path\n",
        "    print(f\"해당하는 json 파일 없음 : {json_path}\")\n",
        "    return None\n",
        "\n",
        "for id in tqdm(embeddings.keys()):\n",
        "    json_path = find_json_by_id(json_root, id)\n",
        "\n",
        "    if json_path is None:\n",
        "        continue  # or log warning\n",
        "\n",
        "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    embeddings[id][\"urgency\"] = data[\"urgencyLevel\"]\n",
        "    embeddings[id][\"sentiment\"] = data[\"sentiment\"]\n",
        "\n",
        "    # print(f\"완료된 id: {id}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZSq-t6wZZm5",
        "outputId": "481c3270-298a-41db-ee29-6aceac909bd5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [57:45<00:00,  2.89it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 최종 임베딩 파일 저장 - total_emb.pkl\n",
        "import pickle\n",
        "\n",
        "with open(\"/content/drive/MyDrive/LikeLion/실전 프로젝트 1/Project/data/emb/total_emb.pkl\", \"wb\") as f:\n",
        "    pickle.dump(embeddings, f)"
      ],
      "metadata": {
        "id": "enmFvbo0i1WY"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface-hub\n",
        "from huggingface_hub import create_repo, upload_file\n",
        "\n",
        "create_repo(\n",
        "    repo_id=\"\"\n",
        ")"
      ],
      "metadata": {
        "id": "SNcPtPa1ny6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with open(\"/content/drive/MyDrive/LikeLion/실전 프로젝트 1/Project/data/emb/total_emb.pkl\", \"rb\") as f:\n",
        "#     embeddings = pickle.load(f)\n",
        "\n",
        "URGENCY_ORDER = dict({\"상\":0, \"중\":1, \"하\":2})\n",
        "SENTIMENT_ORDER = dict({\"당황/난처\":0, \"불안/걱정\":1, \"중립\":2, \"기타부정\":3})\n",
        "\n",
        "train_loader = build_embedding_dataloader(\n",
        "    audio_embeds=[v[\"audio\"] for v in embeddings.values()],\n",
        "    text_embeds=[v[\"text\"] for v in embeddings.values()],\n",
        "    urgencies=[URGENCY_ORDER[v[\"urgency\"]] for v in embeddings.values()],\n",
        "    sentiments=[SENTIMENT_ORDER[v[\"sentiment\"]] for v in embeddings.values()],\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        ")"
      ],
      "metadata": {
        "id": "6hwjjByH-nOZ"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "URGENCY_ORDER = dict({\"상\":0, \"중\":1, \"하\":2})\n",
        "URGENCY_ORDER"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-LpaLkDf8wp",
        "outputId": "94a26bc0-1d64-4722-e3ad-9d4d8f85a692"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'상': 0, '중': 1, '하': 2}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Model, Multitask Loss Initialize"
      ],
      "metadata": {
        "id": "lQ7y-r2M-utv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model\n",
        "model = FusionModel(\n",
        "    urgency_levels=3,\n",
        "    sentiment_levels=4,\n",
        "    fusion_dim=256,\n",
        "    dropout=0.2\n",
        ").to(DEVICE)\n",
        "\n",
        "# MultiTask Loss\n",
        "controller = MultiTaskLossController(\n",
        "    warmup_epochs=1,\n",
        "    urgency_weight=1.0,\n",
        "    sentiment_weight=0.5,\n",
        "    use_uncertainty=False\n",
        ")\n",
        "\n",
        "criterion = MultiTaskLoss(\n",
        "    urgency_loss_fn=ordinal_loss,\n",
        "    sentiment_loss_fn=nn.CrossEntropyLoss(),\n",
        "    controller=controller\n",
        ")\n",
        "\n",
        "# Audio Encoder freeze\n",
        "# for param in model.audio_encoder.parameters():\n",
        "#     param.requires_grad = False\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "grad_monitor = GradMonitor(model)\n",
        "grad_monitor\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHccCoPo-y9w",
        "outputId": "9ec3d5dc-85c4-4e12-83ef-d603586df00e"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<src.core.grad_monitor.GradMonitor at 0x7bf705e6ed80>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Training Loop"
      ],
      "metadata": {
        "id": "wHO6fR-J_BQH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "mWQV6EhL8ytv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in tqdm(range(EPOCHS)):\n",
        "    model.train()\n",
        "    for batch_idx, batch in enumerate(train_loader):\n",
        "        # batch GPU로 이동\n",
        "        for k in batch:\n",
        "            batch[k] = batch[k].to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward\n",
        "        outputs = model(batch)\n",
        "        outputs = {\n",
        "            \"urgency\": outputs[\"urgency\"],\n",
        "            \"sentiment\": outputs[\"sentiment\"],\n",
        "        }\n",
        "\n",
        "        # targets reshape\n",
        "        targets = {\n",
        "            \"urgency\": batch[\"urgency\"].view(-1).float(),\n",
        "            \"sentiment\": batch[\"sentiment\"].view(-1).long()\n",
        "        }\n",
        "\n",
        "        # loss 계산\n",
        "        losses = criterion(outputs, targets, epoch)\n",
        "        # print(losses)\n",
        "        losses[\"total\"].backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        grad_stats = grad_monitor.log(\n",
        "            {\n",
        "                \"urgency\": losses[\"urgency\"],\n",
        "                \"sentiment\": losses[\"sentiment\"],\n",
        "            },\n",
        "            step=batch_idx,\n",
        "            epoch=epoch,\n",
        "        )\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(grad_stats)\n",
        "\n",
        "        if batch_idx % 50 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{EPOCHS}], Batch {batch_idx}, Loss: {losses[\"total\"].item():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a27HMDzX_AT1",
        "outputId": "9ca08985-ab74-4c03-8c4c-e40484e084d3"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/2], Batch 0, Loss: 0.6489\n",
            "Epoch [1/2], Batch 5, Loss: 0.6785\n",
            "Epoch [1/2], Batch 10, Loss: 0.6346\n",
            "Epoch [1/2], Batch 15, Loss: 0.6763\n",
            "Epoch [1/2], Batch 20, Loss: 0.7017\n",
            "Epoch [1/2], Batch 25, Loss: 0.6342\n",
            "Epoch [1/2], Batch 30, Loss: 0.6824\n",
            "Epoch [1/2], Batch 35, Loss: 0.6951\n",
            "Epoch [1/2], Batch 40, Loss: 0.7185\n",
            "Epoch [1/2], Batch 45, Loss: 0.6799\n",
            "Epoch [1/2], Batch 50, Loss: 0.6332\n",
            "Epoch [1/2], Batch 55, Loss: 0.6180\n",
            "Epoch [1/2], Batch 60, Loss: 0.6318\n",
            "Epoch [1/2], Batch 65, Loss: 0.6618\n",
            "Epoch [1/2], Batch 70, Loss: 0.6758\n",
            "Epoch [1/2], Batch 75, Loss: 0.6464\n",
            "Epoch [1/2], Batch 80, Loss: 0.6276\n",
            "Epoch [1/2], Batch 85, Loss: 0.6268\n",
            "Epoch [1/2], Batch 90, Loss: 0.6094\n",
            "Epoch [1/2], Batch 95, Loss: 0.6210\n",
            "Epoch [1/2], Batch 100, Loss: 0.6394\n",
            "Epoch [1/2], Batch 105, Loss: 0.6400\n",
            "Epoch [1/2], Batch 110, Loss: 0.6564\n",
            "Epoch [1/2], Batch 115, Loss: 0.6460\n",
            "Epoch [1/2], Batch 120, Loss: 0.6039\n",
            "Epoch [1/2], Batch 125, Loss: 0.6328\n",
            "Epoch [1/2], Batch 130, Loss: 0.6226\n",
            "Epoch [1/2], Batch 135, Loss: 0.6513\n",
            "Epoch [1/2], Batch 140, Loss: 0.7598\n",
            "Epoch [1/2], Batch 145, Loss: 0.6476\n",
            "Epoch [1/2], Batch 150, Loss: 0.6026\n",
            "Epoch [1/2], Batch 155, Loss: 0.6592\n",
            "Epoch [1/2], Batch 160, Loss: 0.5719\n",
            "Epoch [1/2], Batch 165, Loss: 0.6535\n",
            "Epoch [1/2], Batch 170, Loss: 0.6213\n",
            "Epoch [1/2], Batch 175, Loss: 0.6764\n",
            "Epoch [1/2], Batch 180, Loss: 0.6615\n",
            "Epoch [1/2], Batch 185, Loss: 0.6958\n",
            "Epoch [1/2], Batch 190, Loss: 0.6179\n",
            "Epoch [1/2], Batch 195, Loss: 0.6785\n",
            "Epoch [1/2], Batch 200, Loss: 0.6876\n",
            "Epoch [1/2], Batch 205, Loss: 0.6871\n",
            "Epoch [1/2], Batch 210, Loss: 0.6075\n",
            "Epoch [1/2], Batch 215, Loss: 0.5985\n",
            "Epoch [1/2], Batch 220, Loss: 0.6877\n",
            "Epoch [1/2], Batch 225, Loss: 0.6382\n",
            "Epoch [1/2], Batch 230, Loss: 0.6647\n",
            "Epoch [1/2], Batch 235, Loss: 0.6389\n",
            "Epoch [1/2], Batch 240, Loss: 0.6787\n",
            "Epoch [1/2], Batch 245, Loss: 0.6967\n",
            "Epoch [1/2], Batch 250, Loss: 0.6631\n",
            "Epoch [1/2], Batch 255, Loss: 0.6571\n",
            "Epoch [1/2], Batch 260, Loss: 0.6320\n",
            "Epoch [1/2], Batch 265, Loss: 0.6386\n",
            "Epoch [1/2], Batch 270, Loss: 0.7226\n",
            "Epoch [1/2], Batch 275, Loss: 0.7772\n",
            "Epoch [1/2], Batch 280, Loss: 0.6638\n",
            "Epoch [1/2], Batch 285, Loss: 0.6340\n",
            "Epoch [1/2], Batch 290, Loss: 0.6775\n",
            "Epoch [1/2], Batch 295, Loss: 0.5967\n",
            "Epoch [1/2], Batch 300, Loss: 0.6393\n",
            "Epoch [1/2], Batch 305, Loss: 0.7524\n",
            "Epoch [1/2], Batch 310, Loss: 0.6642\n",
            "Epoch [1/2], Batch 315, Loss: 0.7165\n",
            "Epoch [1/2], Batch 320, Loss: 0.6441\n",
            "Epoch [1/2], Batch 325, Loss: 0.6197\n",
            "Epoch [1/2], Batch 330, Loss: 0.6455\n",
            "Epoch [1/2], Batch 335, Loss: 0.6995\n",
            "Epoch [1/2], Batch 340, Loss: 0.6267\n",
            "Epoch [1/2], Batch 345, Loss: 0.6622\n",
            "Epoch [1/2], Batch 350, Loss: 0.6289\n",
            "Epoch [1/2], Batch 355, Loss: 0.6639\n",
            "Epoch [1/2], Batch 360, Loss: 0.6390\n",
            "Epoch [1/2], Batch 365, Loss: 0.6412\n",
            "Epoch [1/2], Batch 370, Loss: 0.6201\n",
            "Epoch [1/2], Batch 375, Loss: 0.6992\n",
            "Epoch [1/2], Batch 380, Loss: 0.6315\n",
            "Epoch [1/2], Batch 385, Loss: 0.6532\n",
            "Epoch [1/2], Batch 390, Loss: 0.6333\n",
            "Epoch [1/2], Batch 395, Loss: 0.6112\n",
            "Epoch [1/2], Batch 400, Loss: 0.6755\n",
            "Epoch [1/2], Batch 405, Loss: 0.6561\n",
            "Epoch [1/2], Batch 410, Loss: 0.6714\n",
            "Epoch [1/2], Batch 415, Loss: 0.6747\n",
            "Epoch [1/2], Batch 420, Loss: 0.6176\n",
            "Epoch [1/2], Batch 425, Loss: 0.5894\n",
            "Epoch [1/2], Batch 430, Loss: 0.6429\n",
            "Epoch [1/2], Batch 435, Loss: 0.6645\n",
            "Epoch [1/2], Batch 440, Loss: 0.7309\n",
            "Epoch [1/2], Batch 445, Loss: 0.6545\n",
            "Epoch [1/2], Batch 450, Loss: 0.6603\n",
            "Epoch [1/2], Batch 455, Loss: 0.6780\n",
            "Epoch [1/2], Batch 460, Loss: 0.7478\n",
            "Epoch [1/2], Batch 465, Loss: 0.6602\n",
            "Epoch [1/2], Batch 470, Loss: 0.6130\n",
            "Epoch [1/2], Batch 475, Loss: 0.6269\n",
            "Epoch [1/2], Batch 480, Loss: 0.5973\n",
            "Epoch [1/2], Batch 485, Loss: 0.5861\n",
            "Epoch [1/2], Batch 490, Loss: 0.6696\n",
            "Epoch [1/2], Batch 495, Loss: 0.5741\n",
            "Epoch [1/2], Batch 500, Loss: 0.6407\n",
            "Epoch [1/2], Batch 505, Loss: 0.5697\n",
            "Epoch [1/2], Batch 510, Loss: 0.6663\n",
            "Epoch [1/2], Batch 515, Loss: 0.7203\n",
            "Epoch [1/2], Batch 520, Loss: 0.6364\n",
            "Epoch [1/2], Batch 525, Loss: 0.6508\n",
            "Epoch [1/2], Batch 530, Loss: 0.5731\n",
            "Epoch [1/2], Batch 535, Loss: 0.6051\n",
            "Epoch [1/2], Batch 540, Loss: 0.6130\n",
            "Epoch [1/2], Batch 545, Loss: 0.6484\n",
            "Epoch [1/2], Batch 550, Loss: 0.5629\n",
            "Epoch [1/2], Batch 555, Loss: 0.5592\n",
            "Epoch [1/2], Batch 560, Loss: 0.6817\n",
            "Epoch [1/2], Batch 565, Loss: 0.6815\n",
            "Epoch [1/2], Batch 570, Loss: 0.6978\n",
            "Epoch [1/2], Batch 575, Loss: 0.6833\n",
            "Epoch [1/2], Batch 580, Loss: 0.6272\n",
            "Epoch [1/2], Batch 585, Loss: 0.7144\n",
            "Epoch [1/2], Batch 590, Loss: 0.5759\n",
            "Epoch [1/2], Batch 595, Loss: 0.6214\n",
            "Epoch [1/2], Batch 600, Loss: 0.7152\n",
            "Epoch [1/2], Batch 605, Loss: 0.7305\n",
            "Epoch [1/2], Batch 610, Loss: 0.6651\n",
            "Epoch [1/2], Batch 615, Loss: 0.5634\n",
            "Epoch [1/2], Batch 620, Loss: 0.6764\n",
            "Epoch [1/2], Batch 625, Loss: 0.7128\n",
            "Epoch [1/2], Batch 630, Loss: 0.6399\n",
            "Epoch [1/2], Batch 635, Loss: 0.6648\n",
            "Epoch [1/2], Batch 640, Loss: 0.7022\n",
            "Epoch [1/2], Batch 645, Loss: 0.6291\n",
            "Epoch [1/2], Batch 650, Loss: 0.6972\n",
            "Epoch [1/2], Batch 655, Loss: 0.6326\n",
            "Epoch [1/2], Batch 660, Loss: 0.6666\n",
            "Epoch [1/2], Batch 665, Loss: 0.6552\n",
            "Epoch [1/2], Batch 670, Loss: 0.6229\n",
            "Epoch [1/2], Batch 675, Loss: 0.6629\n",
            "Epoch [1/2], Batch 680, Loss: 0.6107\n",
            "Epoch [1/2], Batch 685, Loss: 0.7264\n",
            "Epoch [1/2], Batch 690, Loss: 0.5697\n",
            "Epoch [1/2], Batch 695, Loss: 0.6702\n",
            "Epoch [1/2], Batch 700, Loss: 0.6860\n",
            "Epoch [1/2], Batch 705, Loss: 0.6336\n",
            "Epoch [1/2], Batch 710, Loss: 0.6556\n",
            "Epoch [1/2], Batch 715, Loss: 0.6314\n",
            "Epoch [1/2], Batch 720, Loss: 0.6326\n",
            "Epoch [1/2], Batch 725, Loss: 0.6512\n",
            "Epoch [1/2], Batch 730, Loss: 0.6485\n",
            "Epoch [1/2], Batch 735, Loss: 0.7000\n",
            "Epoch [1/2], Batch 740, Loss: 0.6947\n",
            "Epoch [1/2], Batch 745, Loss: 0.6584\n",
            "Epoch [1/2], Batch 750, Loss: 0.7444\n",
            "Epoch [1/2], Batch 755, Loss: 0.6194\n",
            "Epoch [1/2], Batch 760, Loss: 0.6688\n",
            "Epoch [1/2], Batch 765, Loss: 0.7083\n",
            "Epoch [1/2], Batch 770, Loss: 0.5995\n",
            "Epoch [1/2], Batch 775, Loss: 0.7700\n",
            "Epoch [1/2], Batch 780, Loss: 0.7100\n",
            "Epoch [1/2], Batch 785, Loss: 0.6190\n",
            "Epoch [1/2], Batch 790, Loss: 0.6452\n",
            "Epoch [1/2], Batch 795, Loss: 0.6329\n",
            "Epoch [1/2], Batch 800, Loss: 0.7065\n",
            "Epoch [1/2], Batch 805, Loss: 0.6625\n",
            "Epoch [1/2], Batch 810, Loss: 0.6831\n",
            "Epoch [1/2], Batch 815, Loss: 0.6071\n",
            "Epoch [1/2], Batch 820, Loss: 0.6837\n",
            "Epoch [1/2], Batch 825, Loss: 0.6345\n",
            "Epoch [1/2], Batch 830, Loss: 0.5660\n",
            "Epoch [1/2], Batch 835, Loss: 0.6265\n",
            "Epoch [1/2], Batch 840, Loss: 0.5967\n",
            "Epoch [1/2], Batch 845, Loss: 0.6544\n",
            "Epoch [1/2], Batch 850, Loss: 0.6131\n",
            "Epoch [1/2], Batch 855, Loss: 0.7157\n",
            "Epoch [1/2], Batch 860, Loss: 0.5969\n",
            "Epoch [1/2], Batch 865, Loss: 0.6074\n",
            "Epoch [1/2], Batch 870, Loss: 0.6142\n",
            "Epoch [1/2], Batch 875, Loss: 0.7258\n",
            "Epoch [1/2], Batch 880, Loss: 0.6316\n",
            "Epoch [1/2], Batch 885, Loss: 0.6297\n",
            "Epoch [1/2], Batch 890, Loss: 0.5991\n",
            "Epoch [1/2], Batch 895, Loss: 0.5679\n",
            "Epoch [1/2], Batch 900, Loss: 0.6795\n",
            "Epoch [1/2], Batch 905, Loss: 0.6534\n",
            "Epoch [1/2], Batch 910, Loss: 0.6569\n",
            "Epoch [1/2], Batch 915, Loss: 0.6099\n",
            "Epoch [1/2], Batch 920, Loss: 0.6690\n",
            "Epoch [1/2], Batch 925, Loss: 0.6408\n",
            "Epoch [1/2], Batch 930, Loss: 0.6394\n",
            "Epoch [1/2], Batch 935, Loss: 0.6713\n",
            "Epoch [1/2], Batch 940, Loss: 0.5927\n",
            "Epoch [1/2], Batch 945, Loss: 0.6914\n",
            "Epoch [1/2], Batch 950, Loss: 0.6387\n",
            "Epoch [1/2], Batch 955, Loss: 0.6962\n",
            "Epoch [1/2], Batch 960, Loss: 0.5631\n",
            "Epoch [1/2], Batch 965, Loss: 0.6636\n",
            "Epoch [1/2], Batch 970, Loss: 0.6939\n",
            "Epoch [1/2], Batch 975, Loss: 0.6263\n",
            "Epoch [1/2], Batch 980, Loss: 0.7752\n",
            "Epoch [1/2], Batch 985, Loss: 0.6816\n",
            "Epoch [1/2], Batch 990, Loss: 0.7527\n",
            "Epoch [1/2], Batch 995, Loss: 0.6003\n",
            "Epoch [1/2], Batch 1000, Loss: 0.6744\n",
            "Epoch [1/2], Batch 1005, Loss: 0.5812\n",
            "Epoch [1/2], Batch 1010, Loss: 0.5333\n",
            "Epoch [1/2], Batch 1015, Loss: 0.5778\n",
            "Epoch [1/2], Batch 1020, Loss: 0.7153\n",
            "Epoch [1/2], Batch 1025, Loss: 0.6751\n",
            "Epoch [1/2], Batch 1030, Loss: 0.6156\n",
            "Epoch [1/2], Batch 1035, Loss: 0.6397\n",
            "Epoch [1/2], Batch 1040, Loss: 0.6698\n",
            "Epoch [1/2], Batch 1045, Loss: 0.7201\n",
            "Epoch [1/2], Batch 1050, Loss: 0.6989\n",
            "Epoch [1/2], Batch 1055, Loss: 0.6656\n",
            "Epoch [1/2], Batch 1060, Loss: 0.6184\n",
            "Epoch [1/2], Batch 1065, Loss: 0.6901\n",
            "Epoch [1/2], Batch 1070, Loss: 0.6936\n",
            "Epoch [1/2], Batch 1075, Loss: 0.6796\n",
            "Epoch [1/2], Batch 1080, Loss: 0.6510\n",
            "Epoch [1/2], Batch 1085, Loss: 0.6172\n",
            "Epoch [1/2], Batch 1090, Loss: 0.6829\n",
            "Epoch [1/2], Batch 1095, Loss: 0.6333\n",
            "Epoch [1/2], Batch 1100, Loss: 0.6178\n",
            "Epoch [1/2], Batch 1105, Loss: 0.7386\n",
            "Epoch [1/2], Batch 1110, Loss: 0.7418\n",
            "Epoch [1/2], Batch 1115, Loss: 0.6104\n",
            "Epoch [1/2], Batch 1120, Loss: 0.6853\n",
            "Epoch [1/2], Batch 1125, Loss: 0.6499\n",
            "Epoch [1/2], Batch 1130, Loss: 0.6590\n",
            "Epoch [1/2], Batch 1135, Loss: 0.6643\n",
            "Epoch [1/2], Batch 1140, Loss: 0.6552\n",
            "Epoch [1/2], Batch 1145, Loss: 0.6445\n",
            "Epoch [1/2], Batch 1150, Loss: 0.6421\n",
            "Epoch [1/2], Batch 1155, Loss: 0.5712\n",
            "Epoch [1/2], Batch 1160, Loss: 0.7185\n",
            "Epoch [1/2], Batch 1165, Loss: 0.6360\n",
            "Epoch [1/2], Batch 1170, Loss: 0.6847\n",
            "Epoch [1/2], Batch 1175, Loss: 0.6220\n",
            "Epoch [1/2], Batch 1180, Loss: 0.6411\n",
            "Epoch [1/2], Batch 1185, Loss: 0.5734\n",
            "Epoch [1/2], Batch 1190, Loss: 0.6262\n",
            "Epoch [1/2], Batch 1195, Loss: 0.6088\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 1/2 [00:02<00:02,  2.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/2], Batch 1200, Loss: 0.6725\n",
            "Epoch [1/2], Batch 1205, Loss: 0.6243\n",
            "Epoch [1/2], Batch 1210, Loss: 0.5774\n",
            "Epoch [1/2], Batch 1215, Loss: 0.6043\n",
            "Epoch [1/2], Batch 1220, Loss: 0.5947\n",
            "Epoch [1/2], Batch 1225, Loss: 0.7987\n",
            "Epoch [1/2], Batch 1230, Loss: 0.6477\n",
            "Epoch [1/2], Batch 1235, Loss: 0.6928\n",
            "Epoch [1/2], Batch 1240, Loss: 0.8689\n",
            "Epoch [1/2], Batch 1245, Loss: 0.6516\n",
            "Epoch [2/2], Batch 0, Loss: 1.2310\n",
            "Epoch [2/2], Batch 5, Loss: 1.2919\n",
            "Epoch [2/2], Batch 10, Loss: 1.1624\n",
            "Epoch [2/2], Batch 15, Loss: 1.4262\n",
            "Epoch [2/2], Batch 20, Loss: 1.1671\n",
            "Epoch [2/2], Batch 25, Loss: 1.1718\n",
            "Epoch [2/2], Batch 30, Loss: 1.0725\n",
            "Epoch [2/2], Batch 35, Loss: 1.3649\n",
            "Epoch [2/2], Batch 40, Loss: 1.1904\n",
            "Epoch [2/2], Batch 45, Loss: 1.2488\n",
            "Epoch [2/2], Batch 50, Loss: 1.1319\n",
            "Epoch [2/2], Batch 55, Loss: 1.0347\n",
            "Epoch [2/2], Batch 60, Loss: 1.0858\n",
            "Epoch [2/2], Batch 65, Loss: 1.1440\n",
            "Epoch [2/2], Batch 70, Loss: 1.0037\n",
            "Epoch [2/2], Batch 75, Loss: 1.1734\n",
            "Epoch [2/2], Batch 80, Loss: 1.2048\n",
            "Epoch [2/2], Batch 85, Loss: 1.0141\n",
            "Epoch [2/2], Batch 90, Loss: 1.2164\n",
            "Epoch [2/2], Batch 95, Loss: 1.0879\n",
            "Epoch [2/2], Batch 100, Loss: 1.1829\n",
            "Epoch [2/2], Batch 105, Loss: 1.2134\n",
            "Epoch [2/2], Batch 110, Loss: 1.2151\n",
            "Epoch [2/2], Batch 115, Loss: 1.1645\n",
            "Epoch [2/2], Batch 120, Loss: 1.1705\n",
            "Epoch [2/2], Batch 125, Loss: 1.1361\n",
            "Epoch [2/2], Batch 130, Loss: 1.3735\n",
            "Epoch [2/2], Batch 135, Loss: 0.9985\n",
            "Epoch [2/2], Batch 140, Loss: 1.3913\n",
            "Epoch [2/2], Batch 145, Loss: 1.0837\n",
            "Epoch [2/2], Batch 150, Loss: 1.0737\n",
            "Epoch [2/2], Batch 155, Loss: 1.1352\n",
            "Epoch [2/2], Batch 160, Loss: 1.2516\n",
            "Epoch [2/2], Batch 165, Loss: 1.0269\n",
            "Epoch [2/2], Batch 170, Loss: 1.3443\n",
            "Epoch [2/2], Batch 175, Loss: 1.3445\n",
            "Epoch [2/2], Batch 180, Loss: 1.3725\n",
            "Epoch [2/2], Batch 185, Loss: 1.2544\n",
            "Epoch [2/2], Batch 190, Loss: 1.4400\n",
            "Epoch [2/2], Batch 195, Loss: 1.1225\n",
            "Epoch [2/2], Batch 200, Loss: 1.0042\n",
            "Epoch [2/2], Batch 205, Loss: 1.2709\n",
            "Epoch [2/2], Batch 210, Loss: 1.1600\n",
            "Epoch [2/2], Batch 215, Loss: 1.2701\n",
            "Epoch [2/2], Batch 220, Loss: 1.1186\n",
            "Epoch [2/2], Batch 225, Loss: 1.0351\n",
            "Epoch [2/2], Batch 230, Loss: 1.0842\n",
            "Epoch [2/2], Batch 235, Loss: 1.0651\n",
            "Epoch [2/2], Batch 240, Loss: 1.1145\n",
            "Epoch [2/2], Batch 245, Loss: 1.1399\n",
            "Epoch [2/2], Batch 250, Loss: 1.2433\n",
            "Epoch [2/2], Batch 255, Loss: 1.1603\n",
            "Epoch [2/2], Batch 260, Loss: 1.1689\n",
            "Epoch [2/2], Batch 265, Loss: 1.4326\n",
            "Epoch [2/2], Batch 270, Loss: 1.1521\n",
            "Epoch [2/2], Batch 275, Loss: 1.1677\n",
            "Epoch [2/2], Batch 280, Loss: 1.0687\n",
            "Epoch [2/2], Batch 285, Loss: 1.2459\n",
            "Epoch [2/2], Batch 290, Loss: 1.2237\n",
            "Epoch [2/2], Batch 295, Loss: 1.2097\n",
            "Epoch [2/2], Batch 300, Loss: 1.2670\n",
            "Epoch [2/2], Batch 305, Loss: 1.2321\n",
            "Epoch [2/2], Batch 310, Loss: 1.0896\n",
            "Epoch [2/2], Batch 315, Loss: 1.1194\n",
            "Epoch [2/2], Batch 320, Loss: 1.1058\n",
            "Epoch [2/2], Batch 325, Loss: 0.9923\n",
            "Epoch [2/2], Batch 330, Loss: 1.0745\n",
            "Epoch [2/2], Batch 335, Loss: 1.4068\n",
            "Epoch [2/2], Batch 340, Loss: 1.0078\n",
            "Epoch [2/2], Batch 345, Loss: 1.0379\n",
            "Epoch [2/2], Batch 350, Loss: 1.1520\n",
            "Epoch [2/2], Batch 355, Loss: 1.0497\n",
            "Epoch [2/2], Batch 360, Loss: 1.2217\n",
            "Epoch [2/2], Batch 365, Loss: 0.8928\n",
            "Epoch [2/2], Batch 370, Loss: 1.1629\n",
            "Epoch [2/2], Batch 375, Loss: 1.2744\n",
            "Epoch [2/2], Batch 380, Loss: 1.4554\n",
            "Epoch [2/2], Batch 385, Loss: 1.1650\n",
            "Epoch [2/2], Batch 390, Loss: 1.0344\n",
            "Epoch [2/2], Batch 395, Loss: 1.0961\n",
            "Epoch [2/2], Batch 400, Loss: 0.9669\n",
            "Epoch [2/2], Batch 405, Loss: 1.0836\n",
            "Epoch [2/2], Batch 410, Loss: 1.0531\n",
            "Epoch [2/2], Batch 415, Loss: 1.0101\n",
            "Epoch [2/2], Batch 420, Loss: 1.2855\n",
            "Epoch [2/2], Batch 425, Loss: 1.1533\n",
            "Epoch [2/2], Batch 430, Loss: 1.3930\n",
            "Epoch [2/2], Batch 435, Loss: 1.0196\n",
            "Epoch [2/2], Batch 440, Loss: 1.1262\n",
            "Epoch [2/2], Batch 445, Loss: 1.2519\n",
            "Epoch [2/2], Batch 450, Loss: 1.1077\n",
            "Epoch [2/2], Batch 455, Loss: 1.0824\n",
            "Epoch [2/2], Batch 460, Loss: 1.2815\n",
            "Epoch [2/2], Batch 465, Loss: 1.4814\n",
            "Epoch [2/2], Batch 470, Loss: 1.1777\n",
            "Epoch [2/2], Batch 475, Loss: 1.2946\n",
            "Epoch [2/2], Batch 480, Loss: 1.2222\n",
            "Epoch [2/2], Batch 485, Loss: 1.2197\n",
            "Epoch [2/2], Batch 490, Loss: 1.2097\n",
            "Epoch [2/2], Batch 495, Loss: 1.0395\n",
            "Epoch [2/2], Batch 500, Loss: 1.2238\n",
            "Epoch [2/2], Batch 505, Loss: 1.1868\n",
            "Epoch [2/2], Batch 510, Loss: 1.0301\n",
            "Epoch [2/2], Batch 515, Loss: 1.3107\n",
            "Epoch [2/2], Batch 520, Loss: 1.2843\n",
            "Epoch [2/2], Batch 525, Loss: 1.3481\n",
            "Epoch [2/2], Batch 530, Loss: 1.1911\n",
            "Epoch [2/2], Batch 535, Loss: 1.1763\n",
            "Epoch [2/2], Batch 540, Loss: 1.3046\n",
            "Epoch [2/2], Batch 545, Loss: 0.9695\n",
            "Epoch [2/2], Batch 550, Loss: 1.1269\n",
            "Epoch [2/2], Batch 555, Loss: 1.0863\n",
            "Epoch [2/2], Batch 560, Loss: 1.2456\n",
            "Epoch [2/2], Batch 565, Loss: 0.8870\n",
            "Epoch [2/2], Batch 570, Loss: 1.3869\n",
            "Epoch [2/2], Batch 575, Loss: 1.1749\n",
            "Epoch [2/2], Batch 580, Loss: 1.1519\n",
            "Epoch [2/2], Batch 585, Loss: 1.2325\n",
            "Epoch [2/2], Batch 590, Loss: 1.1952\n",
            "Epoch [2/2], Batch 595, Loss: 1.1014\n",
            "Epoch [2/2], Batch 600, Loss: 1.3667\n",
            "Epoch [2/2], Batch 605, Loss: 1.3567\n",
            "Epoch [2/2], Batch 610, Loss: 1.2585\n",
            "Epoch [2/2], Batch 615, Loss: 1.1390\n",
            "Epoch [2/2], Batch 620, Loss: 1.3781\n",
            "Epoch [2/2], Batch 625, Loss: 1.0850\n",
            "Epoch [2/2], Batch 630, Loss: 1.4510\n",
            "Epoch [2/2], Batch 635, Loss: 1.3060\n",
            "Epoch [2/2], Batch 640, Loss: 1.1813\n",
            "Epoch [2/2], Batch 645, Loss: 1.2977\n",
            "Epoch [2/2], Batch 650, Loss: 1.0415\n",
            "Epoch [2/2], Batch 655, Loss: 1.0221\n",
            "Epoch [2/2], Batch 660, Loss: 1.4074\n",
            "Epoch [2/2], Batch 665, Loss: 1.2581\n",
            "Epoch [2/2], Batch 670, Loss: 1.0419\n",
            "Epoch [2/2], Batch 675, Loss: 1.1234\n",
            "Epoch [2/2], Batch 680, Loss: 1.0326\n",
            "Epoch [2/2], Batch 685, Loss: 1.2720\n",
            "Epoch [2/2], Batch 690, Loss: 1.3461\n",
            "Epoch [2/2], Batch 695, Loss: 1.2389\n",
            "Epoch [2/2], Batch 700, Loss: 1.2981\n",
            "Epoch [2/2], Batch 705, Loss: 1.1349\n",
            "Epoch [2/2], Batch 710, Loss: 1.3215\n",
            "Epoch [2/2], Batch 715, Loss: 1.3631\n",
            "Epoch [2/2], Batch 720, Loss: 0.9539\n",
            "Epoch [2/2], Batch 725, Loss: 1.0013\n",
            "Epoch [2/2], Batch 730, Loss: 1.1717\n",
            "Epoch [2/2], Batch 735, Loss: 1.0601\n",
            "Epoch [2/2], Batch 740, Loss: 1.0754\n",
            "Epoch [2/2], Batch 745, Loss: 1.1488\n",
            "Epoch [2/2], Batch 750, Loss: 1.2151\n",
            "Epoch [2/2], Batch 755, Loss: 1.1441\n",
            "Epoch [2/2], Batch 760, Loss: 0.9263\n",
            "Epoch [2/2], Batch 765, Loss: 0.8947\n",
            "Epoch [2/2], Batch 770, Loss: 1.0188\n",
            "Epoch [2/2], Batch 775, Loss: 0.9409\n",
            "Epoch [2/2], Batch 780, Loss: 1.2580\n",
            "Epoch [2/2], Batch 785, Loss: 1.1298\n",
            "Epoch [2/2], Batch 790, Loss: 1.1370\n",
            "Epoch [2/2], Batch 795, Loss: 1.3017\n",
            "Epoch [2/2], Batch 800, Loss: 1.1758\n",
            "Epoch [2/2], Batch 805, Loss: 1.1507\n",
            "Epoch [2/2], Batch 810, Loss: 1.2442\n",
            "Epoch [2/2], Batch 815, Loss: 1.1264\n",
            "Epoch [2/2], Batch 820, Loss: 1.0794\n",
            "Epoch [2/2], Batch 825, Loss: 1.4640\n",
            "Epoch [2/2], Batch 830, Loss: 0.9369\n",
            "Epoch [2/2], Batch 835, Loss: 1.1599\n",
            "Epoch [2/2], Batch 840, Loss: 1.4013\n",
            "Epoch [2/2], Batch 845, Loss: 1.2522\n",
            "Epoch [2/2], Batch 850, Loss: 1.0965\n",
            "Epoch [2/2], Batch 855, Loss: 1.1932\n",
            "Epoch [2/2], Batch 860, Loss: 1.0852\n",
            "Epoch [2/2], Batch 865, Loss: 1.3570\n",
            "Epoch [2/2], Batch 870, Loss: 1.1113\n",
            "Epoch [2/2], Batch 875, Loss: 1.1855\n",
            "Epoch [2/2], Batch 880, Loss: 1.1397\n",
            "Epoch [2/2], Batch 885, Loss: 1.1367\n",
            "Epoch [2/2], Batch 890, Loss: 1.0499\n",
            "Epoch [2/2], Batch 895, Loss: 1.1387\n",
            "Epoch [2/2], Batch 900, Loss: 1.2183\n",
            "Epoch [2/2], Batch 905, Loss: 1.1105\n",
            "Epoch [2/2], Batch 910, Loss: 1.1700\n",
            "Epoch [2/2], Batch 915, Loss: 1.1016\n",
            "Epoch [2/2], Batch 920, Loss: 1.2142\n",
            "Epoch [2/2], Batch 925, Loss: 1.3643\n",
            "Epoch [2/2], Batch 930, Loss: 1.2384\n",
            "Epoch [2/2], Batch 935, Loss: 1.2225\n",
            "Epoch [2/2], Batch 940, Loss: 1.2179\n",
            "Epoch [2/2], Batch 945, Loss: 1.0667\n",
            "Epoch [2/2], Batch 950, Loss: 1.0105\n",
            "Epoch [2/2], Batch 955, Loss: 1.2808\n",
            "Epoch [2/2], Batch 960, Loss: 1.1721\n",
            "Epoch [2/2], Batch 965, Loss: 1.1633\n",
            "Epoch [2/2], Batch 970, Loss: 1.0841\n",
            "Epoch [2/2], Batch 975, Loss: 1.1009\n",
            "Epoch [2/2], Batch 980, Loss: 1.2500\n",
            "Epoch [2/2], Batch 985, Loss: 1.0421\n",
            "Epoch [2/2], Batch 990, Loss: 1.0931\n",
            "Epoch [2/2], Batch 995, Loss: 1.0176\n",
            "Epoch [2/2], Batch 1000, Loss: 1.2017\n",
            "Epoch [2/2], Batch 1005, Loss: 1.0560\n",
            "Epoch [2/2], Batch 1010, Loss: 1.0856\n",
            "Epoch [2/2], Batch 1015, Loss: 1.4301\n",
            "Epoch [2/2], Batch 1020, Loss: 1.1174\n",
            "Epoch [2/2], Batch 1025, Loss: 1.2270\n",
            "Epoch [2/2], Batch 1030, Loss: 1.2378\n",
            "Epoch [2/2], Batch 1035, Loss: 1.2253\n",
            "Epoch [2/2], Batch 1040, Loss: 1.1476\n",
            "Epoch [2/2], Batch 1045, Loss: 1.0680\n",
            "Epoch [2/2], Batch 1050, Loss: 1.0325\n",
            "Epoch [2/2], Batch 1055, Loss: 1.1838\n",
            "Epoch [2/2], Batch 1060, Loss: 1.3564\n",
            "Epoch [2/2], Batch 1065, Loss: 1.1430\n",
            "Epoch [2/2], Batch 1070, Loss: 0.9987\n",
            "Epoch [2/2], Batch 1075, Loss: 1.2003\n",
            "Epoch [2/2], Batch 1080, Loss: 1.2030\n",
            "Epoch [2/2], Batch 1085, Loss: 1.2374\n",
            "Epoch [2/2], Batch 1090, Loss: 1.0632\n",
            "Epoch [2/2], Batch 1095, Loss: 1.1023\n",
            "Epoch [2/2], Batch 1100, Loss: 1.2029\n",
            "Epoch [2/2], Batch 1105, Loss: 1.3635\n",
            "Epoch [2/2], Batch 1110, Loss: 1.3649\n",
            "Epoch [2/2], Batch 1115, Loss: 1.1771\n",
            "Epoch [2/2], Batch 1120, Loss: 1.1315\n",
            "Epoch [2/2], Batch 1125, Loss: 1.0461\n",
            "Epoch [2/2], Batch 1130, Loss: 1.2514\n",
            "Epoch [2/2], Batch 1135, Loss: 1.1987\n",
            "Epoch [2/2], Batch 1140, Loss: 1.0453\n",
            "Epoch [2/2], Batch 1145, Loss: 0.9848\n",
            "Epoch [2/2], Batch 1150, Loss: 1.1886\n",
            "Epoch [2/2], Batch 1155, Loss: 1.2426\n",
            "Epoch [2/2], Batch 1160, Loss: 1.2245\n",
            "Epoch [2/2], Batch 1165, Loss: 1.0897\n",
            "Epoch [2/2], Batch 1170, Loss: 1.2507\n",
            "Epoch [2/2], Batch 1175, Loss: 1.2174\n",
            "Epoch [2/2], Batch 1180, Loss: 1.1533\n",
            "Epoch [2/2], Batch 1185, Loss: 1.5626\n",
            "Epoch [2/2], Batch 1190, Loss: 1.2025\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:05<00:00,  2.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/2], Batch 1195, Loss: 1.3275\n",
            "Epoch [2/2], Batch 1200, Loss: 1.2108\n",
            "Epoch [2/2], Batch 1205, Loss: 1.2946\n",
            "Epoch [2/2], Batch 1210, Loss: 1.2344\n",
            "Epoch [2/2], Batch 1215, Loss: 1.0923\n",
            "Epoch [2/2], Batch 1220, Loss: 1.3400\n",
            "Epoch [2/2], Batch 1225, Loss: 1.3212\n",
            "Epoch [2/2], Batch 1230, Loss: 1.0349\n",
            "Epoch [2/2], Batch 1235, Loss: 1.2851\n",
            "Epoch [2/2], Batch 1240, Loss: 1.2805\n",
            "Epoch [2/2], Batch 1245, Loss: 1.0486\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Save\n",
        "\n",
        "MODEL_DIR = Path(\"/content/drive/MyDrive/LikeLion/실전 프로젝트 1/Project/models\")\n",
        "\n",
        "# fusion layer\n",
        "torch.save(\n",
        "    model.fusion.state_dict(),\n",
        "    \"fusion_linear.pt\"\n",
        ")\n",
        "\n",
        "# urgency head\n",
        "torch.save(\n",
        "    model.urgency_head.state_dict(),\n",
        "    \"urgency_head.pt\"\n",
        ")\n",
        "\n",
        "# sentiment head\n",
        "torch.save(\n",
        "    model.sentiment_head.state_dict(),\n",
        "    \"sentiment_head.pt\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "usd3VbgUBs8Q"
      },
      "execution_count": 42,
      "outputs": []
    }
  ]
}